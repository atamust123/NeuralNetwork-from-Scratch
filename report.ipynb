{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I: Theory Questions\n",
    "\n",
    "<img src=\"1_2_3.png\">\n",
    "<img src=\"4_5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train set is 39.31%\n",
      "\n",
      "Accuracy of Validation set is 38.67%\n",
      "-------------------------------------------\n",
      "Accuracy of Train set is 45.12%\n",
      "\n",
      "Accuracy of Validation set is 43.80%\n",
      "-------------------------------------------\n",
      "Accuracy of Train set is 35.78%\n",
      "\n",
      "Accuracy of Validation set is 34.00%\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivation(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def ReLu(self, x):\n",
    "        x[x < 0] = 0\n",
    "        return x\n",
    "\n",
    "    def ReLu_derivation(self, x):\n",
    "        x[x > 0] = 1\n",
    "        x[x <= 0] = 0\n",
    "        return x\n",
    "\n",
    "    def mse(self, error):\n",
    "        return np.sum(np.square(error)) / len(error)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    def neg_log_likelihood(self, output, y):  # cross entropy\n",
    "        r = ((y * np.log10(output) + (1 - y) * np.log10(1 - output)) * -1).sum()\n",
    "        return r\n",
    "\n",
    "    def der_cross_entropy(self, output, y):\n",
    "        return output - y\n",
    "\n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def der_tanh(self,x):\n",
    "        return 1/np.square((np.cosh(x)))\n",
    "\n",
    "    def __init__(self, input_layers=900, hidden_layers=[], output_layers=6,\n",
    "                 activation_func=sigmoid, der_func=sigmoid_derivation, loss_func=neg_log_likelihood):\n",
    "        self.input_layers = input_layers\n",
    "        self.hidden_layer = hidden_layers  # when this is empty array it means single layer\n",
    "        self.output_layers = output_layers\n",
    "\n",
    "        self.activation_func = activation_func\n",
    "        self.der_activation_func = der_func\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "        total_layers = [input_layers] + hidden_layers + [output_layers]\n",
    "\n",
    "        weights = list()  # weights are stored for forward and backprop\n",
    "        for i in range(len(total_layers) - 1):\n",
    "            # we create an initial weights.\n",
    "            # weight shape is (900,6) each feature fully connected to the output\n",
    "            weight = np.random.rand(total_layers[i], total_layers[i + 1]) / 1000\n",
    "            # assume that we have input=900 1 hidden=100 output=6\n",
    "            #shape for 1 hidden layer is: first weight is [input layer size=900,first hidden layer=100]\n",
    "                                    # second weight is [first hidden layer=100, output layer size=6]\n",
    "            weights.append(weight)\n",
    "        self.weights = weights\n",
    "\n",
    "        biases = list()\n",
    "        for i in range(len(total_layers) - 1):\n",
    "            bias = np.random.rand(total_layers[i + 1]) / 1000\n",
    "            #shape for 1 hidden layer is: first bias is [first hidden layer=100]\n",
    "                                    # second bias is [output layer size=6]\n",
    "            biases.append(bias)\n",
    "        self.bias = biases\n",
    "\n",
    "        activation_outputs = list()  # sigmoid outputs are stored for backprop\n",
    "        for i in range(len(total_layers)):\n",
    "            activation_outputs.append(np.zeros(total_layers[i]))\n",
    "        self.activation_outputs = activation_outputs\n",
    "\n",
    "        derivation_outputs = list()  # derivations are stored for backprop\n",
    "        for i in range(len(total_layers) - 1):\n",
    "            derivation_outputs.append(np.zeros((total_layers[i], total_layers[i + 1])))\n",
    "        self.derivation_outputs = derivation_outputs\n",
    "\n",
    "        bias_deltas = list() #for bias updates\n",
    "        for i in range(len(total_layers) - 1):\n",
    "            bias_deltas.append(np.zeros(total_layers[i + 1]))\n",
    "        self.bias_deltas = bias_deltas\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input has 900 feature\n",
    "        # activation(3)= sig(h2)\n",
    "        # h2=a(2) x w2 and so on\n",
    "        activations = inputs\n",
    "        self.activation_outputs[0] = activations  # first activation is the input\n",
    "        for i in range(len(self.weights)):\n",
    "            h1 = np.dot(activations, self.weights[i])\n",
    "            h1 += self.bias[i]\n",
    "\n",
    "            activations = self.activation_func(self, h1)\n",
    "            self.activation_outputs[i + 1] = activations  # first weight calculates the second activation output\n",
    "\n",
    "        return self.softmax(activations)\n",
    "\n",
    "    def back_propagation(self, error):\n",
    "        # yhat-y=error[0,-1,0,0,1,0]\n",
    "        for i in range(len(self.derivation_outputs) - 1, -1, -1):\n",
    "            activation_outputs = self.activation_outputs[i + 1]  # these are outputs of layers\n",
    "            delta = error * self.der_activation_func(self, activation_outputs)  # calculate delta\n",
    "            transposed_delta = delta.reshape(delta.shape[0], -1).T  # take transpose to calculate derivation\n",
    "            self.bias_deltas[i] = delta  # bias deriv value\n",
    "\n",
    "            activation_outputs2 = self.activation_outputs[i]  # this is previous layers output\n",
    "            activation_outputs2 = activation_outputs2.reshape(activation_outputs2.shape[0], -1)\n",
    "            self.derivation_outputs[i] = np.dot(activation_outputs2, transposed_delta)  # get the derivation of weight\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "\n",
    "        return error\n",
    "\n",
    "    def gradient_descent(self, learning_rate):  # update values\n",
    "        for i in range(len(self.weights)):\n",
    "            w = self.weights[i]\n",
    "            derivation = self.derivation_outputs[i]\n",
    "            w -= derivation * learning_rate\n",
    "            self.weights[i] = w\n",
    "\n",
    "            self.bias[i] -= self.bias_deltas[i] * learning_rate\n",
    "\n",
    "\n",
    "    def train2(self, train_set, epochs=250, learning_rate=0.05, batch_size=4,decay_rate=0.99):\n",
    "        for i in range(epochs):\n",
    "            total_error = 0\n",
    "            e = 0\n",
    "            j = 0\n",
    "            error = 0\n",
    "            for input, target in train_set:\n",
    "                output = self.forward(input)\n",
    "                y = np.array([1. if x == target else 0. for x in range(len(output))])\n",
    "                error += self.der_cross_entropy(output,y)\n",
    "                loss = self.loss_func(self, output, y)\n",
    "                total_error += loss\n",
    "                if (j + 1) % batch_size == 0:\n",
    "                    self.back_propagation(error / batch_size)\n",
    "                    self.gradient_descent(learning_rate)\n",
    "                    error = 0\n",
    "                elif np.argmax(output) == target:\n",
    "                    e += 1\n",
    "                j += 1\n",
    "            learning_rate *= decay_rate # decay rate\n",
    "            if (i+1) % 50 == 0:\n",
    "                print(e, e / 14034)\n",
    "                print(\"Error: {} at epoch {}\".format(total_error / len(train_set), (i+1)))\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from Layer import NeuralNetwork\n",
    "\n",
    "\n",
    "def read_and_store(path=\"data/seg_train/seg_train\"):\n",
    "    t_set = list()\n",
    "    label_counter = 0  # 0->buildings\n",
    "    # 1-> forest and so on\n",
    "    for train_folder in os.listdir(path):\n",
    "        folder = path + \"\\\\\" + train_folder\n",
    "        for filename in os.listdir(folder):\n",
    "            img = cv2.imread(folder + \"\\\\\" + filename, 0).astype(np.uint8)  # gray img\n",
    "            img = cv2.resize(img, (30, 30), interpolation=cv2.INTER_AREA)\n",
    "            img = (img - np.min(img)) / (np.max(img) - np.min(img))  # Normalization\n",
    "            out = img.flatten()  # to obtain 1d image array\n",
    "            t_set.append([out, label_counter])  # out is 900,1 vector and train_folder is label\n",
    "        label_counter += 1\n",
    "    return t_set\n",
    "\n",
    "\n",
    "def read_test():\n",
    "    test = list()\n",
    "    folder = \"data/seg_test\"\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(folder + \"\\\\\" + filename, 0).astype(np.uint8)  # gray img\n",
    "        img = cv2.resize(img, (30, 30), interpolation=cv2.INTER_AREA)\n",
    "        img = (img - np.min(img)) / (np.max(img) - np.min(img))  # Normalization\n",
    "        out = img.flatten()  # to obtain 1d image array\n",
    "        test.append([out])\n",
    "    return test\n",
    "\n",
    "def calculate_acc(NN,train_set,validation_set):\n",
    "    e = 0\n",
    "    c = 0\n",
    "    for i in range(len(train_set)):\n",
    "        o = NN.forward(train_set[i][0])\n",
    "        error = train_set[i][1] - np.argmax(o)\n",
    "        if error == 0:\n",
    "            c += 1\n",
    "    print(\"Accuracy of Train set is {:.2f}%\".format(c * 100 / len(train_set)))\n",
    "    print()\n",
    "\n",
    "    e = 0\n",
    "    c = 0\n",
    "    for i in range(len(validation_set)):\n",
    "        o = NN.forward(validation_set[i][0])\n",
    "        error = validation_set[i][1] - np.argmax(o)\n",
    "        if error == 0:\n",
    "            c += 1\n",
    "    print(\"Accuracy of Validation set is {:.2f}%\".format(c * 100 / len(validation_set)))\n",
    "    \n",
    "\n",
    "def process(train_set,validation_set):\n",
    "    act_function = [[NeuralNetwork.sigmoid, NeuralNetwork.sigmoid_derivation, \"sigmoid\"],\n",
    "                [NeuralNetwork.tanh, NeuralNetwork.der_tanh, \"tanh\"]]\n",
    "    hid_lay_list = [[], [100], [10, 6]] #first element does not contain, second contains 1 hidden layer,\n",
    "                                    #third contains 2 hidden layers\n",
    "    lr_list = [0.01, 0.05]\n",
    "    batch_list = [2, 4]\n",
    "    counter = 1\n",
    "\n",
    "    for hidden_size in hid_lay_list:\n",
    "        for act, der_act, a in act_function:\n",
    "            for lr in lr_list:\n",
    "                for b_s in batch_list:\n",
    "                    \"\"\"NN = NeuralNetwork(hidden_layers=hidden_size, activation_func=act,\n",
    "                                       der_func=der_act)\n",
    "                    np.random.shuffle(train_set)\n",
    "                    NN.train2(train_set, 200, lr, batch_size=b_s)\n",
    "                    pickle.dump(NN, open((\"model\" + str(counter)), \"wb\"))\n",
    "\n",
    "                    \"\"\"\n",
    "                    NN=pickle.load(open(\"model\"+str(counter),\"rb\"))\n",
    "                    counter += 1\n",
    "\n",
    "\n",
    "                    calculate_acc(NN,train_set,validation_set)\n",
    "                    print(\n",
    "                        \"Model {} Hidden layer {}, Activation function {}, Epoch is 200, Learning Rate is {}, Batch size is {} \".format(\n",
    "                            counter - 1, hidden_size, a, lr, b_s))\n",
    "                    print(\"------------------------------------------------------------------------------\")\n",
    "                    print()\n",
    "\n",
    "\n",
    "\"\"\"train_set = read_and_store(path=\"data/seg_train/seg_train\")\n",
    "test_set = read_test()\n",
    "validation_set = read_and_store(path=\"data/seg_dev/seg_dev\")\n",
    "\n",
    "pickle.dump(train_set,open(\"train_set\", 'wb'))\n",
    "pickle.dump(test_set,open(\"test_set\", 'wb'))\n",
    "pickle.dump(validation_set,open(\"validation\", 'wb'))\n",
    "exit(-1)\"\"\"\n",
    "\n",
    "with open(\"train_set\", \"rb\") as f:\n",
    "    train_set = pickle.load(f)\n",
    "with open(\"test_set\", \"rb\") as f:\n",
    "    test_set = pickle.load(f)\n",
    "\n",
    "with open(\"validation\", \"rb\") as f:\n",
    "    validation_set = pickle.load(f)\n",
    "list=[\"model1_[]_0.001_2_sig\",\"model5_[100]_0001_2_sig\",\"model3_[10_6]_005_2_sig\"]\n",
    "for i in list:    \n",
    "    with open(i,\"rb\") as f:\n",
    "        NN=pickle.load(f)\n",
    "        print()\n",
    "        calculate_acc(NN,train_set,validation_set)\n",
    "        print(\"-------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "process(train_set,validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Classification of Natural Scenes using Neural Network\n",
    "\n",
    "\n",
    "- Many model was created with different hyperparameters. They are stored in pickle files. The code in here used them. Only one Neural Network code is implemented for three types of NN(single layer, 1 hidden layer and 2 hidden layer).\n",
    "\n",
    "## About dataset:\n",
    "\n",
    "  - Data has read one by one, resized 30x30 after that Normalization ((X-x_min)/(x_max-x_min)) was applied. Finally it was converted to 900,1 vector.\n",
    "    - Train set contains [vector,label] vector has 900,1 feature dimension.\n",
    "    \n",
    "    \n",
    "## About Neural Network:\n",
    "- My code default initializes with single layer neural network. Default weight and bias parameters are created with random small values. Then there are 3 backpropagation parameters are defined.(activation_outputs, derivation_outputs, bias_deltas) \n",
    "- There are two activation functions were used (tanh and sigmoid), and sigmoid gives better results.\n",
    "\n",
    "- In general my code above does not give good results.(All three types of network gives around 30% - 40% accuracy.) Even though normalization and avoiding overflow and underflow results are not very good. Nevertheless my network is learning.\n",
    "- About 1 hidden layer neural network: If it contains 100 node in hidden layer it gives 43.8% accuracy. Of course, when the number of nodes are increased, the accuracy also increases. but it does not worth the cost spent.\n",
    "\n",
    "## About forward propagation:\n",
    "    - First activation output is input, it is stored for backpropagation to update the weights and bias. Each time we calculate the next layers input and take its activation output. At the end we returned the softmax function.\n",
    "    \n",
    "## About back propagation:\n",
    "     - The main objective of the project is here. It is important to update all the weights and bias values is cumbersome and took my whole week. When we calculate mini batch error we update the whole weights by derivative of the activation function. In my code activation-outputs are explained in forward.\n",
    "     - After that we update the weights and bias values by the calculated derivation outputs.\n",
    "\n",
    "#### About activation functions:\n",
    " - I firstly wanted to use ReLu, and I already implement it. But it does not converge and i searched for it. In many explanation it is accepted that ReLu is better than both sigmoid and tanh, but it is only one side. For the negative side it is the worst. Some called this dying ReLu.\n",
    " - I chose sigmoid and tanh activation functions. Sigmoid gives slightly better results when I chose small batch size(2,4), when I chose large batch size such as 128 sigmoid and tanh gave me the same accuracy and loss. After each foward propagation I returned softmax.\n",
    " \n",
    " <hr><br>\n",
    " \n",
    "#### Before starting analysis:\n",
    " - Cross entropy(negative log likelihood) was used as loss funciton. Sum of squared used but it does not gives me accurate results and i removed them. Only cross entropy loss will be shown.\n",
    "    \n",
    "  - Smaller learning rate such as 0.01 with decay rate 0.99 gives better results than others. It Ä±s because of oscillation. Learning rate is decresed after each epoch.\n",
    "  - The main criteria of improving the results is that when the batch size is 1 or near to 1 it gives the highest accuracy. But in the plotting I will show the high batch size. (Because I have no time and other lectures have also projects)\n",
    "\n",
    " - I can not find the optimal hidden layers for 2 hidden layer. So I chose smaller and  \n",
    " \n",
    "- Note: The plots below are shown with best accuracy results. For example the highest sigmoid vs the highest tanh.\n",
    "    \n",
    "### Activation functions :\n",
    "    - Sigmoid:\n",
    "<img src=\"m1.png\">\n",
    "    \n",
    "    - Tanh:\n",
    " <img src=\"m2.png\">\n",
    "\n",
    "As you can see the above graph only difference is the activation function and the results are better for sigmoid. Both uses the softmax and cross entropy and 100 epoch. \n",
    " - Loss in sigmoid converges much faster than tanh activation function.\n",
    " - As you can see in the tanh it is affected very much from outliers. \n",
    " - If we change the epoch size and other hyperparameters for optimizing tanh we may obtain better loss and accuracy.\n",
    " - Validation converges to good results after 20. epoch in sigmoid.\n",
    "\n",
    "But sigmoid and tanh have the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Layers:\n",
    "         - Single Layer:\n",
    "  <img src=\"single.png\">       \n",
    "         - One hidden layer:\n",
    "  <img src=\"1_hid.png\">\n",
    "         - Two hidden layer:\n",
    "<img src=\"two_h.png\">\n",
    "\n",
    "All of them uses the same decay rate as lr*=0.99\n",
    "\n",
    "Single layer has 38% accuracy and it is really good for 100 epoch and it does not overfit. Maybe it reach to 70% accuracy with some optimization with hyperparameters but single layer only solves the linearly seperable functions. So that whatever single layer has 99% accuracy this is nothing for real life problems.\n",
    "\n",
    "One hidden layer implementation gives the best results. 100 node used in here but if we give more nodes on hidden layer such as 180 nodes accuracy increases slightly. But it does not worth that for computation cost.\n",
    "\n",
    "Last one contain 2 hidden layer first has 10 nodes and second hidden contains 6 nodes, and it gives good result and does not overfit.\n",
    "\n",
    "### About Loss and Learning Rate:\n",
    "    - In any learning rate such as 0.1,0.01,0.005,0.05 and so on, they always converges to a good or a bad value. But in a different speed. \n",
    "    \n",
    "### Finally:\n",
    "- The results are really bad. But it can be optimized with changing hyperparameters. Choosing different loss function and using softmax did not change result so much.  But batch size affects everything, if batch size is 1 it gives the best accuracy but i did not use 1.(it is not mini batch).\n",
    "- Increasing epoch size can be a good option but i do not think it will reach to 70% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
